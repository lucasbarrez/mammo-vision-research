# Zero-Shot Classification avec Vision-Language Models (CLIP/CPLIP)

> Classification zero-shot de tumeurs mammaires utilisant des modÃ¨les vision-langage (CLIP et CPLIP) avec engineering de prompts

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-orange.svg)](https://pytorch.org/)

## Table des matiÃ¨res

- [Ã€ propos](#Ã -propos)
- [ModÃ¨les](#modÃ¨les)
- [Architecture](#architecture)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)

## Ã€ propos

Ce projet explore l'utilisation de modÃ¨les vision-langage (VLM) pour la classification zero-shot d'images histopathologiques de cancer du sein. Contrairement aux approches traditionnelles qui nÃ©cessitent un entraÃ®nement supervisÃ©, les VLM peuvent classifier des images en utilisant uniquement des descriptions textuelles (prompts).

### Objectifs

- **Tester CLIP** pour la classification zero-shot des 8 types de tumeurs
- **Ã‰valuer CPLIP** (Clinical-CLIP) spÃ©cialisÃ© pour l'imagerie mÃ©dicale
- **Explorer diffÃ©rentes stratÃ©gies de prompting** (simple, descriptif, contextuel)
- **Comparer les performances** avec les approches CNN supervisÃ©es
- **Analyser l'impact du domain-shift** (CLIP gÃ©nÃ©raliste vs CPLIP mÃ©dical)

### CaractÃ©ristiques principales

âœ¨ Classification zero-shot (sans entraÃ®nement)  
ðŸ”¬ ModÃ¨les spÃ©cialisÃ©s mÃ©dical (CPLIP)  
ðŸ“ Multiples stratÃ©gies de prompting  
ðŸ“Š Ã‰valuation complÃ¨te et visualisations  
ðŸ” Analyse de similaritÃ© image-texte  

## ModÃ¨les

### CLIP (Contrastive Language-Image Pre-training)

**OpenAI CLIP** est un modÃ¨le vision-langage prÃ©-entraÃ®nÃ© sur 400M de paires image-texte du web.

- **Architecture**: Vision Transformer (ViT) + Text Transformer
- **PrÃ©-entraÃ®nement**: DonnÃ©es gÃ©nÃ©rales (internet)
- **Forces**: Robustesse, gÃ©nÃ©ralisation
- **Limitations**: Non spÃ©cialisÃ© pour le mÃ©dical

### CPLIP (Clinical Pre-trained Language-Image Pretraining)

**CPLIP** est une variante de CLIP spÃ©cialisÃ©e pour l'imagerie mÃ©dicale.

- **Architecture**: Similaire Ã  CLIP
- **PrÃ©-entraÃ®nement**: DonnÃ©es mÃ©dicales (radiographies, IRM, histopathologie)
- **Forces**: Meilleure comprÃ©hension du vocabulaire mÃ©dical
- **Avantages**: AdaptÃ© au domaine clinique

## Architecture

### Pipeline Zero-Shot

```
Image histopathologique
        â†“
   [Vision Encoder]  â†â†’  [Text Encoder]  â† Prompts textuels
        â†“                      â†“
   Image Features        Text Features
        â†“                      â†“
        â””â”€â”€â”€ Similarity â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
           Classification
```

### StratÃ©gies de Prompting

1. **Simple**: `"A histopathological image of {class_name}"`
2. **Descriptif**: `"A microscopic image showing {class_name}, a type of breast tumor"`
3. **MÃ©dical**: `"Histopathology slide of {class_name} in breast tissue, {characteristics}"`
4. **Contextuel**: Descriptions dÃ©taillÃ©es avec contexte clinique

## Installation

### PrÃ©requis

```bash
python >= 3.8
torch >= 2.0
transformers
PIL
numpy
scikit-learn
matplotlib
```

### Installation des dÃ©pendances

```bash
# Installer les packages
pip install torch torchvision transformers
pip install open-clip-torch  # Pour CLIP
pip install Pillow numpy scikit-learn matplotlib seaborn
```

## Utilisation

### 1. Configuration

Modifiez les paramÃ¨tres dans `config/config.py`:

```python
# ModÃ¨le Ã  utiliser
MODEL_NAME = "clip"  # ou "cplip"
CLIP_MODEL = "ViT-B/32"

# StratÃ©gie de prompting
PROMPT_STRATEGY = "descriptive"  # simple, descriptive, medical, contextual
```

### 2. Lancement de l'Ã©valuation

```bash
python main.py
```

### 3. RÃ©sultats

Les rÃ©sultats sont sauvegardÃ©s dans `results/`:
- Matrices de confusion
- MÃ©triques de classification (accuracy, precision, recall, F1)
- Visualisations de similaritÃ© image-texte
- Comparaison des stratÃ©gies de prompting

## Structure du projet

```
zero_shot_classification/
â”‚
â”œâ”€â”€ README.md                    # Ce fichier
â”œâ”€â”€ main.py                      # Script principal
â”œâ”€â”€ requirements.txt             # DÃ©pendances Python
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py               # Configuration centrale
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ clip_model.py           # Wrapper pour CLIP
â”‚   â”œâ”€â”€ cplip_model.py          # Wrapper pour CPLIP
â”‚   â””â”€â”€ base_vlm.py             # Classe de base VLM
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prompt_templates.py     # Templates de prompts
â”‚   â”œâ”€â”€ prompt_strategies.py    # StratÃ©gies de gÃ©nÃ©ration
â”‚   â””â”€â”€ medical_descriptions.py # Descriptions mÃ©dicales
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset_loader.py       # Chargement du dataset BreakHis
â”‚   â””â”€â”€ preprocessing.py        # PrÃ©traitement des images
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py              # Calcul des mÃ©triques
â”‚   â”œâ”€â”€ visualization.py        # Visualisations
â”‚   â””â”€â”€ comparison.py           # Comparaison des modÃ¨les
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ file_utils.py           # Utilitaires fichiers
â”‚   â””â”€â”€ logging_utils.py        # Logging
â”‚
â”œâ”€â”€ logs/                        # Logs d'exÃ©cution
â””â”€â”€ results/                     # RÃ©sultats et visualisations
```

## ExpÃ©rimentations

### Comparaisons prÃ©vues

1. **CLIP vs CPLIP**: Impact du prÃ©-entraÃ®nement mÃ©dical
2. **StratÃ©gies de prompts**: Simple vs Descriptif vs MÃ©dical
3. **Versions de CLIP**: ViT-B/32 vs ViT-L/14
4. **Zero-shot vs Supervised**: Comparaison avec EfficientNet

### MÃ©triques

- Accuracy globale
- Precision, Recall, F1-score par classe
- Recall sur cancers malins (critique pour le diagnostic)
- Matrice de confusion
- Courbes ROC et AUC

## RÃ©fÃ©rences

### Papers

- **CLIP**: Radford et al. (2021) - "Learning Transferable Visual Models From Natural Language Supervision"
- **CPLIP**: Zhou et al. (2023) - "Clinical-CLIP: Pre-training Language-Image Models for Medical Image Classification"

### Liens

- [OpenAI CLIP](https://github.com/openai/CLIP)
- [OpenCLIP](https://github.com/mlfoundations/open_clip)
- [CPLIP Paper](https://arxiv.org/abs/2301.xxxxx)

## Ã‰tat du projet

- [x] Structure du projet
- [ ] ImplÃ©mentation CLIP
- [ ] ImplÃ©mentation CPLIP
- [ ] GÃ©nÃ©ration de prompts
- [ ] Pipeline d'Ã©valuation
- [ ] ExpÃ©rimentations
- [ ] Analyse des rÃ©sultats
- [ ] RÃ©daction du rapport

## Contributeur

**Alexandre** - Testing CLIP + CPLIP avec prompting

---

*Projet de Computer Vision - Analyse d'images mammographiques*
