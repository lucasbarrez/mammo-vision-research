# Guide de DÃ©marrage Rapide - Zero-Shot Classification avec CLIP/CPLIP

## ğŸš€ Installation

### 1. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# Avec conda
conda create -n vlm-breast python=3.9
conda activate vlm-breast

# Ou avec venv
python -m venv venv
source venv/bin/activate  # Sur macOS/Linux
# venv\Scripts\activate  # Sur Windows
```

### 2. Installer les dÃ©pendances

```bash
cd VLM/zero_shot_classification
pip install -r requirements.txt
```

**Note importante**: Si vous avez un GPU NVIDIA avec CUDA:
```bash
# Installer PyTorch avec support CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“‚ PrÃ©parer le Dataset

Le code s'attend Ã  trouver le dataset BreakHis dans la structure suivante:

```
BreakHis_v1/
â”œâ”€â”€ benign/
â”‚   â”œâ”€â”€ SOB/
â”‚   â”‚   â”œâ”€â”€ adenosis/
â”‚   â”‚   â”‚   â””â”€â”€ 200X/  (ou 40X, 100X, 400X)
â”‚   â”‚   â”œâ”€â”€ fibroadenoma/
â”‚   â”‚   â”œâ”€â”€ tubular_adenoma/
â”‚   â”‚   â””â”€â”€ phyllodes_tumor/
â””â”€â”€ malignant/
    â””â”€â”€ SOB/
        â”œâ”€â”€ ductal_carcinoma/
        â”œâ”€â”€ lobular_carcinoma/
        â”œâ”€â”€ mucinous_carcinoma/
        â””â”€â”€ papillary_carcinoma/
```

**TÃ©lÃ©charger le dataset**: https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/

## ğŸ¯ Utilisation Basique

### Test rapide avec CLIP

```bash
# Ã‰valuation avec CLIP ViT-B/32 et prompts descriptifs
python main.py --model clip --prompt-strategy descriptive --magnification 200

# Options disponibles:
# --model: clip ou cplip
# --clip-variant: ViT-B/32, ViT-B/16, ViT-L/14, RN50, RN101
# --prompt-strategy: simple, descriptive, medical, ensemble
# --magnification: 40, 100, 200, 400
# --device: cuda ou cpu
```

### Exemple complet

```bash
# Ã‰valuation avec CLIP ViT-L/14 (meilleur modÃ¨le)
python main.py \
    --model clip \
    --clip-variant ViT-L/14 \
    --prompt-strategy medical \
    --magnification 200 \
    --batch-size 16 \
    --device cuda
```

## ğŸ“Š RÃ©sultats

Les rÃ©sultats seront sauvegardÃ©s dans `results/`:
- `confusion_matrix_YYYYMMDD_HHMMSS.png` - Matrice de confusion
- `class_metrics_YYYYMMDD_HHMMSS.png` - MÃ©triques par classe
- `results_YYYYMMDD_HHMMSS.json` - RÃ©sultats numÃ©riques

Les logs sont dans `logs/log_YYYYMMDD_HHMMSS.txt`

## ğŸ”¬ ExpÃ©rimentations SuggÃ©rÃ©es

### 1. Tester diffÃ©rents modÃ¨les CLIP

```bash
# ModÃ¨le petit (rapide)
python main.py --clip-variant ViT-B/32

# ModÃ¨le large (meilleur mais plus lent)
python main.py --clip-variant ViT-L/14

# ResNet-based
python main.py --clip-variant RN50
```

### 2. Comparer les stratÃ©gies de prompting

CrÃ©ez un script `compare_strategies.py`:

```python
from config.config import VLMConfig
from data.dataset_loader import BreakHisDataLoader
from models.clip_model import CLIPZeroShot
from evaluation.metrics import Evaluator

# Charger le dataset
data_loader = BreakHisDataLoader(root_dir="./BreakHis_v1", magnification=200)
test_dataset = data_loader.load_test_set()

# Charger le modÃ¨le
model = CLIPZeroShot(model_name="ViT-B/32", device="cuda")

# Comparer les stratÃ©gies
evaluator = Evaluator(model=model, config=VLMConfig)
strategies = ["simple", "descriptive", "medical", "ensemble"]
results = evaluator.compare_strategies(test_dataset, strategies)

# Visualiser la comparaison
from evaluation.visualization import Visualizer
viz = Visualizer(config=VLMConfig)
viz.plot_strategy_comparison(results, save_path="results/strategy_comparison.png")
```

### 3. Ã‰valuation binaire (BÃ©nin vs Malin)

```python
from prompts.prompt_strategies import PromptGenerator
from evaluation.metrics import BinaryEvaluator

# GÃ©nÃ©rer les prompts binaires
prompt_gen = PromptGenerator()
binary_prompts = prompt_gen.generate_binary_prompts()

# Ã‰valuer
binary_eval = BinaryEvaluator(model=model, config=VLMConfig)
binary_results = binary_eval.evaluate_binary(test_dataset, binary_prompts)

print(f"Accuracy binaire: {binary_results['accuracy']:.2%}")
print(f"Recall malins: {binary_results['recall']:.2%}")
```

## ğŸ› Troubleshooting

### Erreur: "No images found"
- VÃ©rifiez que `ROOT_DIR` dans `config/config.py` pointe vers le bon rÃ©pertoire
- VÃ©rifiez que la magnification choisie existe dans le dataset

### Erreur CUDA: "Out of memory"
- RÃ©duisez le batch size: `--batch-size 16` ou `--batch-size 8`
- Utilisez un modÃ¨le plus petit: `--clip-variant ViT-B/32`
- Ou utilisez CPU: `--device cpu`

### Performances faibles
- Essayez diffÃ©rentes stratÃ©gies de prompting
- Testez plusieurs modÃ¨les CLIP
- VÃ©rifiez la qualitÃ© et la distribution des images dans votre dataset

## ğŸ“ TODO / AmÃ©liorations Possibles

- [ ] ImplÃ©menter CPLIP (modÃ¨le mÃ©dical spÃ©cialisÃ©)
- [ ] Ajouter le fine-tuning few-shot
- [ ] ImplÃ©menter l'ensembling de plusieurs modÃ¨les
- [ ] Ajouter des visualisations d'attention
- [ ] Tester avec d'autres datasets mÃ©dicaux
- [ ] Comparer avec les rÃ©sultats CNN (EfficientNet)

## ğŸ“š Ressources

- **CLIP Paper**: https://arxiv.org/abs/2103.00020
- **OpenCLIP**: https://github.com/mlfoundations/open_clip
- **BreakHis Dataset**: https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/

## ğŸ’¡ Tips

1. **Commencez simple**: Testez d'abord avec `ViT-B/32` et `descriptive` prompts
2. **ItÃ©rez sur les prompts**: Le prompting engineering est crucial pour le zero-shot
3. **Analysez les erreurs**: Regardez la matrice de confusion pour identifier les confusions entre classes
4. **Comparez avec baseline**: Comparez les rÃ©sultats avec les CNN supervisÃ©s de votre Ã©quipe

Bonne chance avec vos expÃ©rimentations ! ğŸš€
